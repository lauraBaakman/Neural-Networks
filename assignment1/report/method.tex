%!TEX root = practicum1.tex
Given a dichotomy $\mathcal{D}$ with $N$ $d$-dimensional patterns $\xi \in \mathcal{R}^d$ each with a label $S \in \left\{-1, +1 \right\}$: $\mathcal{D} = \left\{\xi^i, S^i \right\}_{i}^{N}$, a perceptron can be trained using the Rosenblatt algorithm which updates the weights each time step $t = 1, 2, \ldots$:
	\begin{equation}\label{eq:1:rosenblat}
		\vec{w}(t+1) = 
		\begin{cases}
		\vec{w}(t) + \frac{1}{d} \xi^{\mu(t)} S^{\mu(t)}
		& \text{if } E^{\mu(t)} \leq 0\\
		\vec{w}(t) 											
		& \text{otherwise}\\
		\end{cases}
	\end{equation}
Where $\mu(t) = 1, 2, \ldots, N, 1, 2, \ldots$ denotes the present pattern. $E(\cdot)$ the energy function is defined as:
	\begin{equation}\label{eq:1:energyFunction}
		E^{\mu(t)} = \vec{w}(t) \cdot \xi^{\mu(t)}S^{\mu(t)}.
	\end{equation}
The energy function indicates if the perceptron gives the correct output for the given input pattern $\xi^{\mu(t)}$. Since we know that independent of the initial value of the weights the perceptron will converge, if the $\mathcal{D}$ is linearly separable, we can start with $\vec{w} = 0$.\\

The update defined in \autoref{eq:1:rosenblat} is executed until $E^{\xi^i} > 0$ for $i \in [1, N]$ in this cases the algorithm has converged. If the dataset is not linearly separable the perceptron never converges, thus it is generally a good idea to set a maximum number of time steps, $d_{max}$. The total number of steps taken by the algorithm has thus the upper bound $d_{max} \cdot N$. 

To classify a inhomogenously separable dataset with a perceptron one needs to add a one extra input to all patterns, namely minus one and one extra weight, associated with the extra input. Since we can thus classify both homogenously and inhomogenously data sets with a perceptron we will ignore this distinction from here on on.\\

The chance that a randomly chose dichotomy is linearly separable can be proven to be \cite{reed1998neural}:
	\begin{equation}\label{eq:1:lsChance}
		f(N,d) = 
		\begin{cases}
		1
		& N \leq d + 1\\
		\frac{2}{2^N} \sum_{k = 0}^{d} \binom{N - 1}{k}										
		& \text{otherwise}\\
		\end{cases}
	\end{equation}
To verify this theoretical result we have performed two experiments.

\subsection*{Experiment I}
For the first experiment we have generated five hundred 50-dimensional random data sets. 

\subsection*{Experiment II}

\todo[inline]{Experimenten uitleggen}


