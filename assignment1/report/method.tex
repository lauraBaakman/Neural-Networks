%!TEX root = practicum1.tex
Given a dichotomy $\mathcal{D} = \left\{\xi^i, S^i \right\}_{i}^{N}$ with $N$ $d$-dimensional patterns $\xi \in \mathcal{R}^d$, each with a label $S \in \left\{-1, +1 \right\}$, a perceptron can be trained using the Rosenblatt algorithm \todo[inline]{Add a reference to the Rosenblatt algorithm} which updates the weights each time step $t = 1, 2, \ldots$:
	\begin{equation}\label{eq:1:rosenblat}
		\vec{w}(t+1) = 
		\begin{cases}
		\vec{w}(t) + \frac{1}{d} \xi^{\mu(t)} S^{\mu(t)}
		& \text{if } E^{\mu(t)} \leq 0\\
		\vec{w}(t) 											
		& \text{otherwise}\\
		\end{cases}
	\end{equation}
Where $\mu(t) = 1, 2, \ldots, N, 1, 2, \ldots$ is used to select the next pattern to train with. The energy function $E(\cdot)$, anagolous to the local action potential in a biological neuron, is defined as:
	\begin{equation}\label{eq:1:energyFunction}
		E^{\mu(t)} = \vec{w}(t) \cdot \xi^{\mu(t)}S^{\mu(t)}.
	\end{equation}
The energy function indicates if the perceptron gives the correct output for the given input pattern $\xi^{\mu(t)}$. Since we know that independent of the initial value of the weights the perceptron will converge, if the $\mathcal{D}$ is linearly separable, the initialization of the weights is irrelevant. We have opted for $\vec{w} = 0$.\\

The update defined in \autoref{eq:1:rosenblat} is executed until $E^{\xi^i} > 0$ for $i \in [1, N]$, if this is the case the algorithm has converged. If the dataset is not linearly separable the perceptron never converges, thus it is generally a good idea to set a maximum number epochs, $d_{max}$, where each epoch consists of $N$ steps. The total number of steps taken by the algorithm has thus the upper bound $d_{max} \cdot N$. \\


\todo[inline]{Dit stukje naar de intro verplaatsen, bij het linearly separable stukje}
\textcite{cover1965geometrical} showed that the probability that a randomly chosen dichotomy  in general position is linearly separable can be proven to be:
	\begin{equation}\label{eq:1:lsChance}
		f(N,d) = 
		\begin{cases}
		1
		& N \leq d + 1\\
		\frac{2}{2^N} \displaystyle\sum_{k = 0}^{d} \binom{N - 1}{k}										
		& \text{otherwise}\\
		\end{cases}
	\end{equation}
To verify this theoretical result we have performed two experiments.


