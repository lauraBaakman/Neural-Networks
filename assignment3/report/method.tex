%!TEX root = practicum3.tex
We consider a soft-committee machine with weight vectors, $\left\{\vec{w}^1, \, \vec{w}^2 \right\} \in \mathbb{R}^N$, between the $N$ input units and the two hidden units. The soft machine  is a two-layer fully connected network consisting of non-linear hidden units and a linear output unit. The weight values between the hidden units and the output unit are fixed to $+1$. The real-valued output of the network given a pattern $\vec{\xi} \in \mathbb{R}^N$ is defined as:
\begin{equation}\label{eq:1:sigma}
	\sigma(\vec{\xi}) = \left(\tanh\left[\vec{w}^1 \cdot \vec{\xi}\right] + \tanh\left[\vec{w}^2 \cdot \vec{\xi}\right]\right)
\end{equation}

We train the network on a data set with $P$ $N$-dimensional patterns with real valued labels: ${\mathcal{D} = \left\{ \vec{\xi}^\mu, \tau\left(\vec{\xi}^\mu\right) \right\}_{\mu = 1}^{P}}$. The goal of gradient descent training is to minimize the quadratic cost function $E$. 
\begin{equation}\label{eq:1:cost}
	E = \frac{1}{P} \sum_{\mu = 1}^{P} e^\mu
\end{equation}
Where $e^\mu$ is the contribution of one of the $P$ patterns $\xi^\mu$ to the cost function \eqref{eq:1:cost}:
\begin{equation}\label{eq:1:contribution}
	e^\mu = \frac{1}{2}\left(\sigma\left(\vec{\xi}^\mu\right) - \tau\left(\vec{\xi}^\mu\right)\right)^2
\end{equation}
This term is used to update the weight vectors according to:
\begin{align}\label{eq:1:update}
	\vec{w}^m & \leftarrow \vec{w}^m - \eta \nabla \vec{w}^m e^\mu && m \in \{1, 2\}
\end{align}
Where $\eta$ is the learning rate and $\nabla \vec{w}^m e^\mu$ is the gradient of the contribution of one pattern to the cost function \eqref{eq:1:contribution} with respect to the weight vector $\vec{w}^\mu$. 

To determine $\nabla \vec{w}^m e^\mu$ we first determine the derivative of \eqref{eq:1:contribution} with respect to $w_n^m$, the $n$th element of the $m$th weight vector:
\begin{align}\label{eq:1:gradient}
	\frac{\partial e}{\partial w_n^m} & = (\sigma(\vec{\xi}) - \tau(\vec{\xi})) \frac{\partial \sigma}{\partial w_n^m} \nonumber \\
	~& = (\sigma(\vec{\xi}) - \tau(\vec{\xi})) (1 - \tanh^2(\vec{w^m} \cdot \vec{\xi})) \xi_n 
\end{align}

Using the derivation presented in \eqref{eq:1:gradient} we find for $m \in \{1, 2\}$:
\begin{align}\label{eq:1:gradient_vector}
	\nabla \vec{w}^m = \left(\sigma\left(\vec{\xi}\right)- \tau\left(\vec{\xi}\right)\right) \left(1 - \tanh^2\left(\vec{w^m} \cdot \vec{\xi}\right)\right) \vec{\xi}
\end{align}
Note the absence of subscripted $n$ in the last factor of \eqref{eq:1:gradient_vector} when compared with \eqref{eq:1:gradient}.

We have applied \eqref{eq:1:update} in \cref{alg:method}. Although the weight vectors are initialized randomly by \t{init()} they have to satisfy the following requirement: $\norm{\vec{w}}^2 = 1$. This avoids that the weight vectors are initialized too large. Each time step $i$ one pattern is randomly selected to be used by \t{get\_random\_pattern}, as is required for stochastic gradient descent. This algorithm executes $t_{max} \cdot P$ updates on the weight vectors. 

\input{pseudo.tex}

