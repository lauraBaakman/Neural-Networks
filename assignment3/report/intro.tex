%!TEX root = practicum3.tex
In this report we consider a multilayer feed forward neural network, which we will train using the stochastic gradient descent algorithm. Gradient descent (GD) is an optimization algorithm which tries to find the local minimum of a function, by taking steps proportional to the negative of the gradient at a certain point. In the case of a feed forward network gradient descent is applied to minimize the cost function (see \eqref{eq:1:cost} and \cref{s:method} for the explanation of this function). 

The stochastic gradient descent (SGD) algorithm in contrast to the GD algorithm does not use the gradient of the cost function, but the gradient of the contribution (see \eqref{eq:1:contribution}) of a randomly sampled data point. One advantage of SGD, as opposed to gradient descent is that it is faster on very large datasets. Gradient descent goes through the whole training set to perform an update whereas stochastic gradient descent updates the weight vectors iteratively. Each iteration it randomly selects one pattern to be used for updating the weights. One of the advantages to this it that performance of stochastic gradient descent increases each iteration \cite{bottou2010large}.
