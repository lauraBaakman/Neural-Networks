%!TEX root = practicum3.tex
In this report we consider a multilayer feed forward neural network, which we will train using the stochastic gradient descent algorithm. Gradient descent (GD) is an optimization algorithm which tries to find the local minimum of a function, by taking steps proportional to the negative of the gradient at a certain point. In the case of a feed forward network gradient descent is applied to minimize the cost function (see \eqref{eq:1:cost} and \cref{s:method} for the explanation of this function) and thus by doing so hoping to find the solution with the lowest cost. 

The stochastic gradient descent (SGD) algorithm in contrast to the GD algorithm does not use the gradient of the cost function, but the gradient of the contribution (see \eqref{eq:1:contribution} and \cref{s:method} for the explanation) of a randomly sampled data point. The reason why SGD is used apposed to GD is whenever the (training) data set is very large, GD has to go through the whole training set in order to perform an update. SGD on the other hand only takes one data point and updates its weights immediately and thus also immediately starts to increase its performance. 
