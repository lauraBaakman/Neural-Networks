%!TEX root = practicum2.tex
Given a dichotemy $\mathcal{D} = \left\{\xi^i, S^i \right\}_{i}^{P}$ with $P$ $N$-dimensional patterns $\xi \in \mathcal{R}^N$. The labels $S$ were assigned by a teacher perceptron according to:

\begin{equation}\label{eq:method:teacher_label}
	S^\mu = \text{sign}(\vec{w}^* \cdot {\vec{\xi}}^{\mu})
\end{equation}

The teacher $\vec{w}^*$ can be chosen randomly as long as the following condition holds:
\begin{equation}\label{eq:method:teachervectorcondition}
	\norm{\vec{w}^*}^2 = N.
\end{equation}

The Minover algorithm is an iterative procedure that runs over a period of time $t = 0, 1, 2, \dotsc, t_{max}$ until either the stability of the solution does not change anymore for $P$ steps or the number of maximal time steps $t_{max} = n_{max} \cdot P$ has been reached reached. Where $n_{max}$ is comparable to the number of epochs in the Rosenblatt algorithm. The stability of a pattern $\xi$ with label $S$ given a certain weight vector $\vec{w}$ is defined as:

\begin{equation}\label{eq:method:maximum_stability}
	\mathnormal{k} = \frac{\vec{w} \cdot \vec{\xi} \cdot S}{\norm{\vec{w}}}.
\end{equation}

We say that the stability has converged when the last $P$ calculated generalization errors\eqref{eq:method:generalization_error} differentiate less than $0 \pm \varepsilon$.\\

The stability defined in \eqref{eq:method:maximum_stability} can be thought of as the distance between all the patterns and the current solution $\vec{w}(t)$. To eventually find the weight vector with maximal stability the algorithm takes the following two steps every iteration: it selects the pattern $\mu(t)$ with the minimal distance/stability (minimal overlap) to the current solution. The current weight vector $\vec{w}(t)$ is then updated with Hebb's rule, see \eqref{eq:method:update}.

\begin{equation}\label{eq:method:update}
	\vec{w}(t + 1) = \vec{w}(t) + \frac{1}{N} \xi^{\mu(t)} S^{\mu(t)} 
\end{equation}

See \autoref{alg:method:minnover} for the pseudo code of the procedure described above. The method \texttt{notConverged} compares the generalization error \eqref{eq:method:generalization_error} of the last $P$ iterations, as explained earlier. 

\input{pseudo}

The earlier mentioned generalization error $\epsilon$ gives the probability that the teacher vector $\vec{w}^*$ and the student vector $\vec{w}$ disagree:

\begin{equation}\label{eq:method:generalization_error}
	\epsilon = \frac{2\phi}{2\pi} = \frac{1}{\pi} \arccos \left(\frac{\vec{w} \cdot \vec{w}^*}{\norm{\vec{w}} \norm{\vec{w}^*}}\right).
\end{equation}

The parameter $\phi$ mentioned iin \eqref{eq:method:generalization_error} is the angle between the teacher and student vector, shown in \cref{fig:methode:generalizationError}. The probability of a different classification by the two vectors is indicated by the shaded area of the unit circle in \cref{fig:methode:generalizationError}.\\

\begin{figure}
	\centering
	\includegraphics[scale=1]{./img/generalizationError}
	\caption{The difference between the student vector $\vec{w}$ and the weight vector $\vec{w}^*$.}
	\label{fig:methode:generalizationError}
\end{figure}

As stated before Minover contrary to Rosenblatt does not stop when a solution is found but continues until the solution with optimal stability is found.

